#!/usr/bin/env python3
# ============================================================================
# Crypto System v2 - Multi-Crypto Training with YFinance (Day + 1h)
# ============================================================================\n\nimport os\nimport time\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, LSTM, Dense, Dropout, Concatenate, Conv1D, BatchNormalization\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.preprocessing import StandardScaler\n\n# Install required packages\nprint(\"Installing dependencies...\")\nos.system('pip install yfinance -q')\n\nimport yfinance as yf\n\nprint(\"=\"*80)\nprint(\"CRYPTO SYSTEM v2 - MULTI-CRYPTO TRAINING (YFINANCE)\")\nprint(\"Cryptocurrencies × 2 Timeframes (1d + 1h) × 7000+ Candles\")\nprint(\"=\"*80 + \"\\n\")\n\n# Setup GPU\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    print(f\"\u2713 GPU: {len(gpus)} device(s)\")\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"\u26a0 No GPU detected\")\n\n# Output directory\nOUTPUT_DIR = Path(\"/content/all_models/multi_crypto\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nprint(f\"\u2713 Output directory: {OUTPUT_DIR}\\n\")\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCRYPTOS = [\n    ('BTC', 'BTC-USD'),\n    ('ETH', 'ETH-USD'),\n    ('BNB', 'BNB-USD'),\n    ('SOL', 'SOL-USD'),\n    ('XRP', 'XRP-USD'),\n    ('ADA', 'ADA-USD'),\n    ('DOGE', 'DOGE-USD'),\n    ('AVAX', 'AVAX-USD'),\n    ('LTC', 'LTC-USD'),\n    ('DOT', 'DOT-USD'),\n    ('UNI', 'UNI-USD'),\n    ('LINK', 'LINK-USD'),\n    ('XLM', 'XLM-USD'),\n    ('ATOM', 'ATOM-USD'),\n]\n\n# Use 1d (daily) + 1h (hourly) instead of 1h + 15m\n# 1d: 3000+ days available\n# 1h: 400+ days available\nTIMEFRAMES = {\n    '1d': '1d',\n    '1h': '1h',\n}\n\nLOOKBACK = 60\nEPOCHS = 40\nBATCH_SIZE = 32\nEARLY_STOP_PATIENCE = 8\n\nprint(f\"Cryptocurrencies: {len(CRYPTOS)}\")\nprint(f\"Timeframes: {list(TIMEFRAMES.keys())}\")\nprint(f\"Total models to train: {len(CRYPTOS) * len(TIMEFRAMES)}\")\nprint(f\"Lookback period: {LOOKBACK} candles\\n\")\n\n# ============================================================================\n# FEATURE ENGINEERING\n# ============================================================================\n\ndef engineer_features(df):\n    df = df.copy()\n    \n    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n    df['simple_return'] = (df['close'] - df['close'].shift(1)) / df['close'].shift(1)\n    \n    df['volatility'] = df['log_return'].rolling(14).std()\n    df['volatility_20'] = df['log_return'].rolling(20).std()\n    \n    df['momentum_10'] = df['close'] - df['close'].shift(10)\n    df['momentum_20'] = df['close'] - df['close'].shift(20)\n    \n    df['price_range'] = (df['high'] - df['low']) / df['close']\n    \n    delta = df['close'].diff()\n    gain = delta.where(delta > 0, 0).rolling(14).mean()\n    loss = -delta.where(delta < 0, 0).rolling(14).mean()\n    rs = gain / (loss + 1e-8)\n    df['rsi'] = 100 - (100 / (1 + rs))\n    \n    ema_12 = df['close'].ewm(span=12).mean()\n    ema_26 = df['close'].ewm(span=26).mean()\n    df['macd'] = ema_12 - ema_26\n    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n    \n    df['volume_ma'] = df['volume'].rolling(20).mean()\n    df['volume_ratio'] = df['volume'] / (df['volume_ma'] + 1e-8)\n    \n    df['high_low'] = df['high'] - df['low']\n    df['high_close'] = abs(df['high'] - df['close'].shift(1))\n    df['low_close'] = abs(df['low'] - df['close'].shift(1))\n    df['true_range'] = df[['high_low', 'high_close', 'low_close']].max(axis=1)\n    df['atr'] = df['true_range'].rolling(14).mean()\n    \n    df['funding_rate'] = df['momentum_10'].rolling(20).mean() / (df['close'].rolling(20).std() + 1e-8) * 0.00001\n    df['open_interest_change'] = (df['volume_ratio'] - 1) * (df['volatility'] / 0.01) * 0.1\n    \n    df = df.ffill().bfill()\n    df = df.replace([np.inf, -np.inf], 0)\n    \n    return df\n\n# ============================================================================\n# FETCH DATA WITH YFINANCE\n# ============================================================================\n\ndef fetch_crypto_data(symbol, yfinance_ticker, interval):\n    try:\n        print(f\"    Fetching {symbol} {interval}...\", end=\" \")\n        \n        if interval == '1d':\n            days_back = 3000  # ~3000 days for daily data\n        else:  # 1h\n            days_back = 400   # ~400 days for hourly data\n        \n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=days_back)\n        \n        # Fetch data with error suppression\n        df = yf.download(\n            yfinance_ticker,\n            start=start_date.date(),\n            end=end_date.date(),\n            interval=interval,\n            progress=False,\n            prepost=False,\n            threads=False\n        )\n        \n        if df is None or len(df) == 0:\n            print(\"NO DATA\")\n            return None\n        \n        # Handle MultiIndex columns (from yfinance)\n        if isinstance(df.columns, pd.MultiIndex):\n            df.columns = df.columns.get_level_values(0)\n        \n        # Rename and clean columns\n        df.columns = df.columns.str.lower()\n        df = df.reset_index()\n        \n        # Standardize timestamp column name\n        if 'date' in df.columns:\n            df = df.rename(columns={'date': 'timestamp'})\n        elif 'datetime' in df.columns:\n            df = df.rename(columns={'datetime': 'timestamp'})\n        \n        # Ensure required columns exist\n        required = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n        missing = [c for c in required if c not in df.columns]\n        \n        if missing:\n            print(f\"MISSING: {missing}\")\n            return None\n        \n        # Select and clean data\n        df = df[required].copy()\n        df = df.dropna()\n        df = df[df['volume'] > 0]  # Remove zero-volume bars\n        \n        print(f\"{len(df)} candles\")\n        return df\n        \n    except Exception as e:\n        error_msg = str(e)[:60]\n        print(f\"ERROR: {error_msg}\")\n        return None\n\n# ============================================================================\n# BUILD LSTM MODEL\n# ============================================================================\n\ndef build_lstm_model(n_features, lookback):\n    input_main = Input(shape=(lookback, n_features), name='input_main')\n    input_aux = Input(shape=(lookback, n_features), name='input_aux')\n    \n    x1 = LSTM(64, activation='relu', return_sequences=True)(input_main)\n    x1 = BatchNormalization()(x1)\n    x1 = Dropout(0.2)(x1)\n    x1 = LSTM(32, activation='relu', return_sequences=False)(x1)\n    x1 = Dropout(0.2)(x1)\n    \n    x2 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(input_aux)\n    x2 = BatchNormalization()(x2)\n    x2 = Dropout(0.2)(x2)\n    x2 = LSTM(32, activation='relu', return_sequences=False)(x2)\n    x2 = Dropout(0.2)(x2)\n    \n    combined = Concatenate()([x1, x2])\n    z = Dense(32, activation='relu')(combined)\n    z = BatchNormalization()(z)\n    z = Dropout(0.2)(z)\n    z = Dense(16, activation='relu')(z)\n    z = Dropout(0.1)(z)\n    output = Dense(1, name='return')(z)\n    \n    model = Model(inputs=[input_main, input_aux], outputs=output)\n    model.compile(\n        optimizer=Adam(learning_rate=0.0003, clipvalue=1.0),\n        loss='huber',\n        metrics=['mae']\n    )\n    \n    return model\n\n# ============================================================================\n# TRAIN SINGLE CRYPTO-TIMEFRAME PAIR\n# ============================================================================\n\ndef train_crypto_model(symbol, yfinance_ticker, interval):\n    print(f\"\\n{'='*80}\")\n    print(f\"Training: {symbol} {interval}\")\n    print(f\"{'='*80}\")\n    \n    # 1. Fetch data\n    print(f\"  Phase 1: Fetching data...\")\n    df = fetch_crypto_data(symbol, yfinance_ticker, interval)\n    \n    if df is None or len(df) < LOOKBACK + 100:\n        print(f\"  SKIP: Not enough data ({len(df) if df is not None else 0} candles)\")\n        return False, None\n    \n    # 2. Feature engineering\n    print(f\"  Phase 2: Computing features...\", end=\" \")\n    df = engineer_features(df)\n    print(\"Done\")\n    \n    # 3. Create sequences\n    print(f\"  Phase 3: Creating sequences...\", end=\" \")\n    feature_cols = [\n        'open', 'high', 'low', 'close', 'volume',\n        'log_return', 'volatility', 'volatility_20', 'momentum_10', 'momentum_20',\n        'price_range', 'rsi', 'macd', 'macd_signal', 'volume_ratio', 'atr',\n        'funding_rate', 'open_interest_change'\n    ]\n    \n    X, y, base_prices = [], [], []\n    for i in range(len(df) - LOOKBACK - 1):\n        X.append(df[feature_cols].iloc[i:i+LOOKBACK].values)\n        y.append(df['log_return'].iloc[i+LOOKBACK])\n        base_prices.append(df['close'].iloc[i+LOOKBACK-1])\n    \n    X = np.array(X)\n    y = np.array(y)\n    base_prices = np.array(base_prices)\n    print(f\"{len(X)} sequences\")\n    \n    # 4. Time-series split\n    n_total = len(X)\n    n_train = int(n_total * 0.70)\n    n_val = int(n_total * 0.15)\n    \n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_val = X[n_train:n_train+n_val]\n    y_val = y[n_train:n_train+n_val]\n    X_test = X[n_train+n_val:]\n    y_test = y[n_train+n_val:]\n    base_prices_test = base_prices[n_train+n_val:]\n    \n    # 5. Normalize\n    print(f\"  Phase 4: Normalizing...\", end=\" \")\n    X_train_reshaped = X_train.reshape(-1, X_train.shape[2])\n    scaler_X = StandardScaler()\n    scaler_X.fit(X_train_reshaped)\n    \n    X_train_norm = X_train.copy()\n    X_val_norm = X_val.copy()\n    X_test_norm = X_test.copy()\n    \n    for i in range(len(X_train_norm)):\n        X_train_norm[i] = scaler_X.transform(X_train[i])\n    for i in range(len(X_val_norm)):\n        X_val_norm[i] = scaler_X.transform(X_val[i])\n    for i in range(len(X_test_norm)):\n        X_test_norm[i] = scaler_X.transform(X_test[i])\n    \n    scaler_y = StandardScaler()\n    scaler_y.fit(y_train.reshape(-1, 1))\n    y_train_norm = scaler_y.transform(y_train.reshape(-1, 1)).flatten()\n    y_val_norm = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n    print(\"Done\")\n    \n    # 6. Train model\n    print(f\"  Phase 5: Training model...\", end=\" \")\n    model = build_lstm_model(X_train_norm.shape[2], LOOKBACK)\n    \n    start_time = time.time()\n    history = model.fit(\n        [X_train_norm, X_train_norm], y_train_norm,\n        validation_data=([X_val_norm, X_val_norm], y_val_norm),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=[\n            EarlyStopping(monitor='val_loss', patience=EARLY_STOP_PATIENCE, restore_best_weights=True, verbose=0),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=0),\n        ],\n        verbose=0\n    )\n    train_time = time.time() - start_time\n    print(f\"{train_time:.0f}s\")\n    \n    # 7. Evaluate\n    print(f\"  Phase 6: Evaluating...\", end=\" \")\n    y_pred_norm = model.predict([X_test_norm, X_test_norm], verbose=0).flatten()\n    y_pred = scaler_y.inverse_transform(y_pred_norm.reshape(-1, 1)).flatten()\n    \n    price_pred = base_prices_test * np.exp(y_pred)\n    price_actual = base_prices_test * np.exp(y_test)\n    mape = np.mean(np.abs((price_actual - price_pred) / price_actual)) * 100\n    direction_acc = np.sum((y_test * y_pred) > 0) / len(y_test) * 100\n    \n    print(f\"MAPE: {mape:.4f}%, Dir.Acc: {direction_acc:.2f}%\")\n    \n    # 8. Save model and scalers\n    print(f\"  Phase 7: Saving...\", end=\" \")\n    \n    model_filename = f\"{symbol}_{interval}_model.h5\"\n    scalers_filename = f\"{symbol}_{interval}_scalers.pkl\"\n    \n    model_path = OUTPUT_DIR / model_filename\n    scalers_path = OUTPUT_DIR / scalers_filename\n    \n    model.save(str(model_path))\n    \n    scalers_dict = {\n        'X': scaler_X,\n        'y': scaler_y,\n        'feature_cols': feature_cols,\n        'lookback': LOOKBACK\n    }\n    with open(scalers_path, 'wb') as f:\n        pickle.dump(scalers_dict, f)\n    \n    print(f\"Done\")\n    \n    # Clean up\n    import gc\n    gc.collect()\n    tf.keras.backend.clear_session()\n    \n    return True, mape\n\n# ============================================================================\n# MAIN TRAINING LOOP\n# ============================================================================\n\nprint(\"Starting training...\\n\")\n\nresults = []\ntotal_models = len(CRYPTOS) * len(TIMEFRAMES)\ncurrent_model = 0\n\nfor symbol, yfinance_ticker in CRYPTOS:\n    for interval_name, interval_code in TIMEFRAMES.items():\n        current_model += 1\n        print(f\"[{current_model}/{total_models}] {symbol} {interval_name}\")\n        \n        success, mape = train_crypto_model(symbol, yfinance_ticker, interval_code)\n        \n        if success:\n            results.append({\n                'symbol': symbol,\n                'timeframe': interval_name,\n                'mape': mape,\n                'status': 'SUCCESS'\n            })\n        else:\n            results.append({\n                'symbol': symbol,\n                'timeframe': interval_name,\n                'mape': None,\n                'status': 'SKIPPED'\n            })\n\n# ============================================================================\n# SUMMARY\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\"*80 + \"\\n\")\n\ndf_results = pd.DataFrame(results)\n\nprint(\"Results by Status:\")\nfor status in df_results['status'].unique():\n    count = len(df_results[df_results['status'] == status])\n    print(f\"  {status}: {count}\")\n\nprint(f\"\\nSuccessful Models: {len(df_results[df_results['status'] == 'SUCCESS'])}\")\n\nif len(df_results[df_results['status'] == 'SUCCESS']) > 0:\n    print(f\"\\nMAPE Statistics:\")\n    mape_data = df_results[df_results['status'] == 'SUCCESS']['mape']\n    print(f\"  Mean MAPE: {mape_data.mean():.4f}%\")\n    print(f\"  Median MAPE: {mape_data.median():.4f}%\")\n    print(f\"  Best MAPE: {mape_data.min():.4f}%\")\n    print(f\"  Worst MAPE: {mape_data.max():.4f}%\")\n    print(f\"\\nTop 5 models:\")\n    top_5 = df_results[df_results['status'] == 'SUCCESS'].nsmallest(5, 'mape')\n    for _, row in top_5.iterrows():\n        print(f\"    {row['symbol']:6} {row['timeframe']:3} - MAPE: {row['mape']:.4f}%\")\n\nprint(f\"\\nAll models saved to: {OUTPUT_DIR}\")\nprint(f\"Total files created: {len(list(OUTPUT_DIR.glob('*')))}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\u2713 TRAINING COMPLETE!\")\nprint(\"=\"*80)\n